# MyLLM - Production-Grade Local LLM Runtime

A custom-built local LLM runtime system, designed for learning, extensibility, and production use. Run large language models locally with streaming chat, embeddings, and a clean REST API + CLI interface.

## Features

- ðŸš€ **Local Inference**: Run LLMs locally using llama.cpp with GGUF models
- ðŸ’¬ **Chat API**: Streaming chat with conversation history management
- ðŸ”„ **Generation**: Single-shot text generation
- ðŸŽ¯ **Embeddings**: Generate text embeddings
- âš¡ **GPU Acceleration**: Automatic GPU detection and layer offloading
- ðŸ“ **Session Management**: Persistent conversation history with context window truncation
- ðŸŒŠ **Streaming**: Server-Sent Events (SSE) for real-time token streaming
- ðŸ› ï¸ **CLI**: Interactive chat and server management
- ðŸŽ¨ **Clean Architecture**: Separated concerns, extensible, maintainable

## Architecture

```
Client (CLI/API) â†’ API Layer (FastAPI) â†’ Service Layer â†’ Engine Layer (llama.cpp)
                                      â†“
                              Storage Layer (SQLite)
```

**Key Components**:
- **API Layer**: REST endpoints for chat, generate, models, embeddings
- **Core**: Runtime orchestration, session management, prompt building
- **Engine**: llama.cpp wrapper with streaming support
- **Services**: Model loading, inference execution, embeddings
- **Storage**: SQLite for session persistence

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/myllm.git
cd myllm

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -e .

# For GPU support (CUDA example)
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --force-reinstall --no-cache-dir

# For Mac (Metal)
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
```

### Setup

1. **Configure environment**:
```bash
cp .env.example .env
# Edit .env to set MODELS_DIR, PORT, etc.
```

2. **Download a model** (manually for now):
```bash
mkdir -p models_data/llama-3-8b
cd models_data/llama-3-8b

# Download GGUF model (example)
wget https://huggingface.co/.../model.gguf

# Create config.json
cat > config.json << EOF
{
  "name": "llama-3-8b",
  "family": "llama",
  "quantization": "Q4_K_M",
  "context_size": 8192,
  "template": "llama3",
  "parameters": {
    "temperature": 0.7,
    "top_p": 0.9
  }
}
EOF
```

### Usage

#### Start API Server

```bash
myllm serve
# Server runs at http://localhost:8000
# API docs at http://localhost:8000/docs
```

#### Interactive Chat (CLI)

```bash
myllm run llama-3-8b
```

```
You: Hello!
Assistant: Hello! How can I help you today?

You: What's the capital of France?
Assistant: The capital of France is Paris.

You: /exit
Goodbye!
```

#### API Usage

**Chat (Streaming)**:
```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3-8b",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "stream": true
  }'
```

Response (SSE stream):
```
data: {"token":"Hello","done":false}

data: {"token":"!","done":false}

data: {"done":true,"session_id":"123e4567-e89b-12d3-a456-426614174000","full_text":"Hello! How can I help you today?"}
```

**Chat (Non-Streaming)**:
```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3-8b",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "stream": false
  }'
```

**Generate**:
```bash
curl -X POST http://localhost:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3-8b",
    "prompt": "Once upon a time",
    "options": {
      "max_tokens": 100,
      "temperature": 0.8
    }
  }'
```

**List Models**:
```bash
curl http://localhost:8000/api/models
```

**Embeddings**:
```bash
curl -X POST http://localhost:8000/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3-8b",
    "input": "Hello world"
  }'
```

## Project Structure

```
myllm/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI application factory
â”‚   â”œâ”€â”€ api/                    # REST API endpoints
â”‚   â”‚   â”œâ”€â”€ chat.py            # Chat endpoint with streaming
â”‚   â”‚   â”œâ”€â”€ generate.py        # Single-shot generation
â”‚   â”‚   â”œâ”€â”€ embeddings.py      # Embeddings endpoint
â”‚   â”‚   â””â”€â”€ models.py          # Model management
â”‚   â”œâ”€â”€ core/                   # Core business logic
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”‚   â”œâ”€â”€ runtime.py         # Runtime orchestrator
â”‚   â”‚   â”œâ”€â”€ session.py         # Session & history management
â”‚   â”‚   â””â”€â”€ prompt.py          # Prompt template builder
â”‚   â”œâ”€â”€ engine/                 # Inference engine layer
â”‚   â”‚   â”œâ”€â”€ llama_cpp.py       # llama.cpp wrapper
â”‚   â”‚   â”œâ”€â”€ tokenizer.py       # Token counting
â”‚   â”‚   â””â”€â”€ streaming.py       # SSE streaming handler
â”‚   â”œâ”€â”€ models/                 # Model management
â”‚   â”‚   â”œâ”€â”€ registry.py        # Model discovery & registry
â”‚   â”‚   â””â”€â”€ schemas.py         # Pydantic data models
â”‚   â”œâ”€â”€ services/               # Service layer
â”‚   â”‚   â”œâ”€â”€ model_loader.py    # Model loading with caching
â”‚   â”‚   â”œâ”€â”€ inference.py       # Inference orchestration
â”‚   â”‚   â””â”€â”€ embeddings.py      # Embedding generation
â”‚   â”œâ”€â”€ storage/                # Data persistence
â”‚   â”‚   â”œâ”€â”€ database.py        # SQLite ORM models
â”‚   â”‚   â””â”€â”€ cache.py           # In-memory caching
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚       â”œâ”€â”€ hardware.py        # Hardware detection
â”‚       â”œâ”€â”€ logging.py         # Logging configuration
â”‚       â””â”€â”€ errors.py          # Custom exceptions
â”œâ”€â”€ cli/                        # CLI interface
â”‚   â”œâ”€â”€ main.py                # CLI entry point (Typer)
â”‚   â””â”€â”€ commands/              # CLI commands
â”‚       â”œâ”€â”€ pull.py            # Download models
â”‚       â”œâ”€â”€ run.py             # Interactive chat
â”‚       â””â”€â”€ serve.py           # Start API server
â”œâ”€â”€ models_data/                # Model storage
â”‚   â””â”€â”€ <model_name>/
â”‚       â”œâ”€â”€ model.gguf
â”‚       â””â”€â”€ config.json
â”œâ”€â”€ tests/                      # Test suite
â”œâ”€â”€ scripts/                    # Utility scripts
â””â”€â”€ docs/                       # Documentation
```

## Configuration

Edit `.env`:

```bash
# Server
HOST=127.0.0.1
PORT=8000
LOG_LEVEL=INFO

# Paths
MODELS_DIR=./models_data
DB_PATH=./myllm.db

# Inference
DEFAULT_CONTEXT_SIZE=4096
DEFAULT_N_GPU_LAYERS=-1        # -1 = use all GPU layers
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_TOKENS=512

# Performance
MAX_LOADED_MODELS=3            # LRU cache size
```

## Model Configuration

Each model needs a `config.json`:

```json
{
  "name": "llama-3-8b",
  "family": "llama",
  "quantization": "Q4_K_M",
  "context_size": 8192,
  "template": "llama3",
  "parameters": {
    "temperature": 0.7,
    "top_p": 0.9,
    "repeat_penalty": 1.1
  }
}
```

**Supported Templates**:
- `llama3` - Llama 3 format
- `chatml` - ChatML format (default)
- `alpaca` - Alpaca format
- `vicuna` - Vicuna format

## Development

### Setup Dev Environment

```bash
pip install -e ".[dev]"
```

### Run Tests

```bash
pytest
pytest --cov=app tests/
```

### Code Quality

```bash
# Format
black app/ cli/ tests/

# Lint
ruff check app/ cli/ tests/

# Type check
mypy app/ cli/
```

## API Reference

### Chat Endpoint

**POST /api/chat**

Request:
```typescript
{
  model: string;
  messages: Array<{role: "system" | "user" | "assistant", content: string}>;
  session_id?: string;    // Optional: resume conversation
  stream?: boolean;       // Default: true
  options?: {
    temperature?: number;
    top_p?: number;
    max_tokens?: number;
    stop?: string[];
  };
}
```

Response (streaming):
```
data: {"token": "...", "done": false}
data: {"done": true, "session_id": "...", "full_text": "..."}
```

Response (non-streaming):
```json
{
  "message": {"role": "assistant", "content": "..."},
  "session_id": "...",
  "usage": {"prompt_tokens": 10, "completion_tokens": 15, "total_tokens": 25}
}
```

### Generate Endpoint

**POST /api/generate**

Request:
```json
{
  "model": "llama-3-8b",
  "prompt": "Once upon a time",
  "stream": false,
  "options": {
    "max_tokens": 100,
    "temperature": 0.8
  }
}
```

### Models Endpoint

**GET /api/models**

Returns list of available models.

**GET /api/models/{model_name}**

Returns detailed model information.

## Performance Tips

1. **GPU Offloading**: Set `n_gpu_layers=-1` to offload all layers to GPU
2. **Quantization**: Use Q4_K_M or Q5_K_M for good balance of speed/quality
3. **Context Size**: Reduce `context_size` if running out of VRAM
4. **Batch Size**: Adjust `n_batch` in llama.cpp for your hardware
5. **KV Cache**: Enable by default for faster multi-turn conversations

## Troubleshooting

### Model Not Loading

- Check model path in `MODELS_DIR`
- Verify `config.json` exists
- Check llama.cpp Python bindings installed correctly

### GPU Not Detected

- Reinstall llama-cpp-python with correct CMAKE flags
- Check CUDA/Metal/ROCm drivers installed
- Verify `nvidia-smi` or equivalent shows GPU

### Out of Memory

- Reduce `n_gpu_layers`
- Use more aggressive quantization (Q4_0)
- Reduce `context_size`
- Limit `MAX_LOADED_MODELS`

## Roadmap

- [ ] Automatic model downloads from Hugging Face
- [ ] Vision model support (LLaVA)
- [ ] Function calling / tools
- [ ] Multi-model routing
- [ ] RAG (retrieval-augmented generation)
- [ ] Model fine-tuning integration
- [ ] Distributed inference
- [ ] Web UI dashboard

## License

MIT

## Contributing

Contributions welcome! Please read CONTRIBUTING.md first.

## Acknowledgments

- **llama.cpp**: Incredible inference engine
- **FastAPI**: Modern Python web framework
- **Ollama**: Inspiration for this project
